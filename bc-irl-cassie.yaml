env_settings: {}
obs_shape: ???
action_dim: ???
total_num_updates: ???
action_is_discrete: ???
num_steps: 5
num_envs: 16
device: cpu
only_eval: false
seed: 3
num_eval_episodes: 100
num_env_steps: 5000000
recurrent_hidden_state_size: 128
gamma: 0.99
log_interval: 10
eval_interval: 500
save_interval: 10000000000
load_checkpoint: null
load_policy: true
resume_training: false
policy:
    _target_: imitation_learning.policy_opt.policy.Policy
    hidden_size: 128
    recurrent_hidden_size: 128
    is_recurrent: false
    obs_shape: ${obs_shape}
    action_dim: ${action_dim}
    action_is_discrete: ${action_is_discrete}
    std_init: 0
    num_envs: ${num_envs}
policy_updater:
    _target_: imitation_learning.bcirl.BCIRL
    _recursive_: false
    use_clipped_value_loss: true
    clip_param: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.0001
    max_grad_norm: 0.5
    num_epochs: 2
    num_mini_batch: 16
    num_envs: ${num_envs}
    num_steps: ${num_steps}
    gae_lambda: 0.95
    gamma: ${gamma}
    optimizer_params:
        _target_: torch.optim.Adam
        lr: 0.003
    batch_size: 50000
    plot_interval: ${eval_interval}
    norm_expert_actions: false
    n_inner_iters: 10
    reward_update_freq: 1
    device: ${device}
    total_num_updates: ${total_num_updates}
    use_lr_decay: false
    get_dataset_fn:
        _target_: imitation_learning.common.utils.get_transition_dataset
        dataset_path: ""
        env_name: ${env.env_name}
    policy_init_fn:
        _target_: imitation_learning.bcirl.reg_init
        _recursive_: false
        policy_cfg: ${policy}
    reward:
        _target_: imitation_learning.common.rewards.NeuralReward
        obs_shape: ${obs_shape}
        action_dim: ${action_dim}
        reward_hidden_dim: 128
        reward_type: NEXT_STATE
        cost_take_dim: -1
        include_tanh: false
        n_hidden_layers: 2
    inner_updater:
        _target_: imitation_learning.policy_opt.ppo.PPO
        _recursive_: false
        use_clipped_value_loss: true
        max_grad_norm: -1
        value_loss_coef: 0.5
        clip_param: 0.3
        entropy_coef: 0.001
        num_epochs: 2
        num_mini_batch: 16
        num_envs: ${num_envs}
        num_steps: ${num_steps}
        gae_lambda: 0.95
        gamma: 0.99
    inner_opt:
        _target_: torch.optim.Adam
        lr: 0.0001
    reward_opt:
        _target_: torch.optim.Adam
        lr: 0.001
    irl_loss:
        _target_: torch.nn.MSELoss
        reduction: mean
should_load_opt: true
logger:
    _target_: rl_utils.logging.Logger
    _recursive_: false
    run_name: ""
    seed: ${seed}
    log_dir: ./data/vids/
    vid_dir: ./data/vids/
    save_dir: ./data/checkpoints/
    smooth_len: 10
    group_name: ""
env:
    env_name: cassie-v0
    env_settings:
        params:
            _target_: cassie.CassieEnv
            config: {}
        set_eval: false
