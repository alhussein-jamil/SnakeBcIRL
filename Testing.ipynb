{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "from typing import Dict,List\n",
    "\n",
    "import gym.spaces as spaces\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from hydra.utils import instantiate as hydra_instantiate\n",
    "from omegaconf import DictConfig\n",
    "from rl_utils.envs import create_vectorized_envs\n",
    "from rl_utils.logging import Logger\n",
    "from tensordict.tensordict import TensorDict\n",
    "from torchrl.envs.utils import step_mdp\n",
    "from typing import Tuple\n",
    "from imitation_learning.common.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_env import SnakeEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "cfg = yaml.load(open(\"bc-irl-snake.yaml\", 'r'), Loader=yaml.SafeLoader)\n",
    "cfg = DictConfig(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env_settings': {}, 'obs_shape': '???', 'action_dim': '???', 'total_num_updates': '???', 'action_is_discrete': '???', 'num_steps': 50, 'num_envs': 256, 'device': 'cpu', 'only_eval': False, 'seed': 3, 'num_eval_episodes': 100, 'num_env_steps': 30000000, 'recurrent_hidden_state_size': 128, 'gamma': 0.8, 'log_interval': 10, 'eval_interval': 500, 'save_interval': 10000000000, 'load_checkpoint': None, 'load_policy': True, 'resume_training': False, 'policy': {'_target_': 'imitation_learning.policy_opt.policy.Policy', 'hidden_size': 512, 'recurrent_hidden_size': 128, 'is_recurrent': False, 'obs_shape': '${obs_shape}', 'action_dim': '${action_dim}', 'action_is_discrete': '${action_is_discrete}', 'std_init': 0, 'num_envs': '${num_envs}'}, 'policy_updater': {'_target_': 'imitation_learning.bcirl.BCIRL', '_recursive_': False, 'use_clipped_value_loss': True, 'clip_param': 0.2, 'value_loss_coef': 0.5, 'entropy_coef': 0.0001, 'max_grad_norm': 0.5, 'num_epochs': 2, 'num_mini_batch': 4, 'num_envs': '${num_envs}', 'num_steps': '${num_steps}', 'gae_lambda': 0.95, 'gamma': '${gamma}', 'optimizer_params': {'_target_': 'torch.optim.Adam', 'lr': 0.0003}, 'batch_size': 256, 'plot_interval': '${eval_interval}', 'norm_expert_actions': False, 'n_inner_iters': 1, 'reward_update_freq': 1, 'device': '${device}', 'total_num_updates': '${total_num_updates}', 'use_lr_decay': False, 'get_dataset_fn': {'_target_': 'imitation_learning.common.utils.get_transition_dataset', 'dataset_path': 'traj/pm_100.pth', 'env_name': '${env.env_name}'}, 'policy_init_fn': {'_target_': 'imitation_learning.bcirl.reg_init', '_recursive_': False, 'policy_cfg': '${policy}'}, 'reward': {'_target_': 'imitation_learning.common.rewards.NeuralReward', 'obs_shape': '${obs_shape}', 'action_dim': '${action_dim}', 'reward_hidden_dim': 128, 'reward_type': 'NEXT_STATE', 'cost_take_dim': -1, 'include_tanh': False, 'n_hidden_layers': 2}, 'inner_updater': {'_target_': 'imitation_learning.policy_opt.ppo.PPO', '_recursive_': False, 'use_clipped_value_loss': True, 'max_grad_norm': -1, 'value_loss_coef': 0.5, 'clip_param': 0.2, 'entropy_coef': 0.001, 'num_epochs': 2, 'num_mini_batch': 4, 'num_envs': '${num_envs}', 'num_steps': '${num_steps}', 'gae_lambda': 0.95, 'gamma': 0.99}, 'inner_opt': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'reward_opt': {'_target_': 'torch.optim.Adam', 'lr': 0.001}, 'irl_loss': {'_target_': 'torch.nn.MSELoss', 'reduction': 'mean'}}, 'logger': {'_target_': 'rl_utils.logging.Logger', '_recursive_': False, 'run_name': '', 'seed': '${seed}', 'log_dir': './data/vids/', 'vid_dir': './data/vids/', 'save_dir': './data/checkpoints/', 'smooth_len': 10, 'group_name': ''}, 'env': {'env_name': 'snakie-v0', 'env_settings': {'params': {'_target_': 'snake_env.SnakeEnv', 'config': {'render_mode': 'rgb_array', 'screen_width': 200, 'screen_height': 200, 'block_size': 20, 'max_hunger_coef': 1, 'max_steps_coef': 30, 'num_exp': 20}}, 'set_eval': False}}, 'evaluator': {'_target_': 'imitation_learning.common.pointmass_utils.PointMassVisualizer', 'rnn_hxs_dim': '${policy.recurrent_hidden_size}', 'num_render': 0, 'fps': 10, 'save_traj_name': None, 'plt_lim': 1.5, 'plt_density': 50, 'agent_point_size': 60, 'num_demo_plot': 10, 'plot_il': True, 'with_arrows': False, 'plot_expert': True, 'is_final_render': False}, 'eval_args': {'policy_updater': {'reward_update_freq': -1, 'n_inner_iters': 1}, 'load_policy': False, 'num_env_steps': 5000000}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the seed for numpy, python random, and pytorch.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "class vectorized_env():\n",
    "    def __init__(self, envs : List[Env]):\n",
    "        self.envs = envs\n",
    "        self.num_envs = len(self.envs)\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space = self.envs[0].action_space\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        return torch.tensor([env.reset()[0].tolist() for env in self.envs],dtype=torch.float32)\n",
    "    \n",
    "    def step(self, action) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[Dict]]:\n",
    "        steps = [env.step(action[i]) for i,env in enumerate(self.envs)]\n",
    "        return_value = (torch.tensor([step[0].tolist() for step in steps],dtype=torch.float32),\n",
    "                torch.tensor([step[1] for step in steps],dtype=torch.float32),\n",
    "                torch.tensor([step[2] for step in steps],dtype=torch.bool),\n",
    "                [step[3] for step in steps])\n",
    "        return return_value\n",
    "    \n",
    "    def render(self, mode = \"rgb_array\"):\n",
    "        if(self.envs[0].render_mode == \"rgb_array\"):\n",
    "            return [env.render(mode) for env in self.envs]\n",
    "        else:\n",
    "            self.envs[0].render(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning full prefix 65-3-aJmVtQ\n",
      "policy {'_target_': 'imitation_learning.policy_opt.policy.Policy', 'hidden_size': 512, 'recurrent_hidden_size': 128, 'is_recurrent': False, 'obs_shape': [4], 'action_dim': 4, 'action_is_discrete': False, 'std_init': 0, 'num_envs': 256}\n",
      "policy_updater {'_target_': 'imitation_learning.bcirl.BCIRL', '_recursive_': False, 'use_clipped_value_loss': True, 'clip_param': 0.2, 'value_loss_coef': 0.5, 'entropy_coef': 0.0001, 'max_grad_norm': 0.5, 'num_epochs': 2, 'num_mini_batch': 4, 'num_envs': '${num_envs}', 'num_steps': '${num_steps}', 'gae_lambda': 0.95, 'gamma': '${gamma}', 'optimizer_params': {'_target_': 'torch.optim.Adam', 'lr': 0.0003}, 'batch_size': 256, 'plot_interval': '${eval_interval}', 'norm_expert_actions': False, 'n_inner_iters': 1, 'reward_update_freq': 1, 'device': '${device}', 'total_num_updates': '${total_num_updates}', 'use_lr_decay': False, 'get_dataset_fn': {'_target_': 'imitation_learning.common.utils.get_transition_dataset', 'dataset_path': 'traj/pm_100.pth', 'env_name': '${env.env_name}'}, 'policy_init_fn': {'_target_': 'imitation_learning.bcirl.reg_init', '_recursive_': False, 'policy_cfg': '${policy}'}, 'reward': {'_target_': 'imitation_learning.common.rewards.NeuralReward', 'obs_shape': '${obs_shape}', 'action_dim': '${action_dim}', 'reward_hidden_dim': 128, 'reward_type': 'NEXT_STATE', 'cost_take_dim': -1, 'include_tanh': False, 'n_hidden_layers': 2}, 'inner_updater': {'_target_': 'imitation_learning.policy_opt.ppo.PPO', '_recursive_': False, 'use_clipped_value_loss': True, 'max_grad_norm': -1, 'value_loss_coef': 0.5, 'clip_param': 0.2, 'entropy_coef': 0.001, 'num_epochs': 2, 'num_mini_batch': 4, 'num_envs': '${num_envs}', 'num_steps': '${num_steps}', 'gae_lambda': 0.95, 'gamma': 0.99}, 'inner_opt': {'_target_': 'torch.optim.Adam', 'lr': 0.0001}, 'reward_opt': {'_target_': 'torch.optim.Adam', 'lr': 0.001}, 'irl_loss': {'_target_': 'torch.nn.MSELoss', 'reduction': 'mean'}}\n",
      "observations\n",
      "1672\n",
      "actions\n",
      "1672\n",
      "terminals\n",
      "1672\n",
      "next_observations\n",
      "1672\n",
      "rewards\n",
      "1672\n",
      "infos\n",
      "1672\n",
      "observations\n",
      "\t torch.Size([1672, 4])\n",
      "actions\n",
      "\t torch.Size([1672, 4])\n",
      "terminals\n",
      "\t torch.Size([1672])\n",
      "next_observations\n",
      "\t torch.Size([1672, 4])\n",
      "rewards\n",
      "\t torch.Size([1672])\n",
      "infos\n",
      "\t 1672\n"
     ]
    }
   ],
   "source": [
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "\n",
    "# Setup the environments\n",
    "set_env_settings = {\n",
    "    k: hydra_instantiate(v) if isinstance(v, DictConfig) else v\n",
    "    for k, v in cfg.env.env_settings.items()\n",
    "}\n",
    "envs = vectorized_env([SnakeEnv(cfg.env.env_settings.params.config) for _ in range(cfg.num_envs)])\n",
    "\n",
    "steps_per_update = cfg.num_steps * cfg.num_envs\n",
    "\n",
    "num_updates = int(cfg.num_env_steps) // steps_per_update\n",
    "\n",
    "# Set dynamic variables in the config.\n",
    "cfg.obs_shape = envs.observation_space.shape\n",
    "cfg.action_dim = envs.action_space.shape[0]\n",
    "cfg.action_is_discrete = isinstance(cfg.action_dim, spaces.Discrete)\n",
    "cfg.total_num_updates = num_updates\n",
    "\n",
    "logger: Logger = hydra_instantiate(cfg.logger, full_cfg=cfg)\n",
    "print(\"policy\",cfg.policy)\n",
    "policy = hydra_instantiate(cfg.policy)\n",
    "policy = policy.to(device)\n",
    "print(\"policy_updater\",cfg.policy_updater)\n",
    "updater = hydra_instantiate(cfg.policy_updater, policy=policy, device=device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_update = 0\n",
    "if cfg.load_checkpoint is not None:\n",
    "    # Load a checkpoint for the policy/reward. Also potentially resume\n",
    "    # training.\n",
    "    ckpt = torch.load(cfg.load_checkpoint)\n",
    "    updater.load_state_dict(ckpt[\"updater\"], should_load_opt=cfg.resume_training)\n",
    "    if cfg.load_policy:\n",
    "        policy.load_state_dict(ckpt[\"policy\"])\n",
    "    if cfg.resume_training:\n",
    "        start_update = ckpt[\"update_i\"] + 1\n",
    "\n",
    "eval_info = {\"run_name\": logger.run_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updates 0, Steps 12800, FPS 2800\n",
      "Over the last 10 episodes:\n",
      "    - episode.reward: -0.15801115251422143\n",
      "    - episode.score: 0.0\n",
      "    - episode.distance_to_goal: 0.4444028851440428\n",
      "    - inferred_episode_reward: 0.7712286368012429\n",
      "    - value_loss: 0.29877109453082085\n",
      "    - action_loss: -0.010995903692673892\n",
      "    - dist_entropy: 5.675752639770508\n",
      "    - irl_loss: 0.22453956305980682\n",
      "\n",
      "Updates 10, Steps 140800, FPS 4365\n",
      "Over the last 10 episodes:\n",
      "    - episode.reward: -0.2522793998092183\n",
      "    - episode.score: 0.0\n",
      "    - episode.distance_to_goal: 0.377349639981784\n",
      "    - inferred_episode_reward: 11.18510410785675\n",
      "    - value_loss: 14.09570255279541\n",
      "    - action_loss: -0.0020032030995935203\n",
      "    - dist_entropy: 5.676194190979004\n",
      "    - irl_loss: 0.22218220978975295\n",
      "\n",
      "Updates 20, Steps 268800, FPS 4380\n",
      "Over the last 10 episodes:\n",
      "    - episode.reward: -0.2558032473069802\n",
      "    - episode.score: 0.0\n",
      "    - episode.distance_to_goal: 0.4473079018129836\n",
      "    - inferred_episode_reward: 6.824732875823974\n",
      "    - value_loss: 27.982652854919433\n",
      "    - action_loss: -0.00252932240255177\n",
      "    - dist_entropy: 5.67535400390625\n",
      "    - irl_loss: 0.21579414308071138\n",
      "\n",
      "Updates 30, Steps 396800, FPS 4374\n",
      "Over the last 10 episodes:\n",
      "    - episode.reward: -0.38379843011684744\n",
      "    - episode.score: 0.0\n",
      "    - episode.distance_to_goal: 0.4022011924711089\n",
      "    - inferred_episode_reward: 12.621129965782165\n",
      "    - value_loss: 31.481301498413085\n",
      "    - action_loss: -0.008652000717120245\n",
      "    - dist_entropy: 5.676154136657715\n",
      "    - irl_loss: 0.22079529911279677\n",
      "\n",
      "Updates 40, Steps 524800, FPS 4336\n",
      "Over the last 10 episodes:\n",
      "    - episode.reward: -0.13745115987669693\n",
      "    - episode.score: 0.0\n",
      "    - episode.distance_to_goal: 0.26253391131888437\n",
      "    - inferred_episode_reward: 16.589993238449097\n",
      "    - value_loss: 28.126787376403808\n",
      "    - action_loss: -0.007733765023294837\n",
      "    - dist_entropy: 5.675592422485352\n",
      "    - irl_loss: 0.2607361406087875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Storage for the rollouts\n",
    "obs = envs.reset()\n",
    "td = TensorDict({\"observation\": obs}, batch_size=[cfg.num_envs])\n",
    "\n",
    "# Storage for the rollouts\n",
    "storage_td = TensorDict({}, batch_size=[cfg.num_envs, cfg.num_steps], device=device)\n",
    "\n",
    "for update_i in range(start_update, num_updates):\n",
    "    is_last_update = update_i == num_updates - 1\n",
    "    for step_idx in range(cfg.num_steps):\n",
    "\n",
    "        # Collect experience.\n",
    "        with torch.no_grad():\n",
    "            policy.act(td)\n",
    "        next_obs, reward, done, infos = envs.step(td[\"action\"])\n",
    "\n",
    "        td[\"next_observation\"] = next_obs\n",
    "        for env_i, info in enumerate(infos):\n",
    "            if \"final_obs\" in info:\n",
    "                td[\"next_observation\"][env_i] = info[\"final_obs\"]\n",
    "        td[\"reward\"] = reward.reshape(-1, 1)\n",
    "        td[\"done\"] = done\n",
    "    \n",
    "        storage_td[:, step_idx] = td\n",
    "        # Log to CLI/wandb.\n",
    "        logger.collect_env_step_info(infos)\n",
    "    \n",
    "    # Call method specific update function\n",
    "    updater.update(policy, storage_td, logger, envs=envs)\n",
    "\n",
    "\n",
    "\n",
    "    if cfg.log_interval != -1 and (\n",
    "        update_i % cfg.log_interval == 0 or is_last_update\n",
    "    ):\n",
    "        logger.interval_log(update_i, steps_per_update * (update_i + 1))\n",
    "        height = 2\n",
    "        width = 2\n",
    "        eval_env = SnakeEnv(cfg.env.env_settings.params.config)\n",
    "        fig, ax = plt.subplots(nrows=height, ncols=width, sharex=True, sharey=True, gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "\n",
    "        last_reward_map = np.zeros((eval_env.screen_width//eval_env.block_size, eval_env.screen_height//eval_env.block_size))\n",
    "\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                \n",
    "                reward_map = np.zeros((eval_env.screen_width//eval_env.block_size, eval_env.screen_height//eval_env.block_size))\n",
    "                apple_pos = eval_env.reset()[0][:2]\n",
    "                #test what you got so far by plotting a heat map of the reward using the snake only \n",
    "                for x in range(eval_env.screen_width//eval_env.block_size):\n",
    "                    for y in range(eval_env.screen_height//eval_env.block_size ):\n",
    "                        x_grid = x * eval_env.block_size / eval_env.screen_width\n",
    "                        y_grid = y * eval_env.block_size / eval_env.screen_height\n",
    "                        reward_map[x,y] = updater.reward(next_obs = torch.tensor([*apple_pos,x_grid,y_grid]+[0]*(eval_env.observation_space.shape[0]-4),dtype=torch.float32).to(device).view(1,1,-1))\n",
    "                # for x in range(eval_env.screen_width//eval_env.block_size):\n",
    "                #     for y in range(eval_env.screen_height//eval_env.block_size ):\n",
    "                #         print(f\"{reward_map[x,y]:.2f}\", end=\" \")\n",
    "                #     print()\n",
    "                    \n",
    "                # Define the color map\n",
    "                cmap = plt.cm.get_cmap('hot')\n",
    "\n",
    "                # Plot the reward map without axis and numbers\n",
    "                image = ax[i,j].imshow(reward_map, cmap=cmap, interpolation='nearest')\n",
    "                ax[i,j].axis('off')\n",
    "\n",
    "                # Plot the apple\n",
    "                ax[i,j].scatter(\n",
    "                    apple_pos[1] * eval_env.screen_height // eval_env.block_size,\n",
    "                    apple_pos[0] * eval_env.screen_width // eval_env.block_size,\n",
    "                    c='blue',\n",
    "                    s=60\n",
    "                )\n",
    "                # map_diff = reward_map - last_reward_map\n",
    "                # for x in range(eval_env.screen_width//eval_env.block_size):\n",
    "                #     for y in range(eval_env.screen_height//eval_env.block_size ):\n",
    "                #          print(f\"{reward_map[x,y]:.2f}\", end=\" \")\n",
    "                #     print()\n",
    "                    \n",
    "                # print(\"reward_maps diff \" ,np.linalg.norm(reward_map-last_reward_map))\n",
    "                last_reward_map = reward_map\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(osp.join(logger.save_path, f\"reward_map.{update_i}.png\"))\n",
    "        print(f\"Saved to {osp.join(logger.save_path, f'reward_map.{update_i}.png')}\")\n",
    "        # for x in range(eval_env.screen_width//eval_env.block_size):\n",
    "        #     for y in range(eval_env.screen_height//eval_env.block_size ):\n",
    "        #         print(f\"{reward_map[x,y]:.2f}\", end=\" \")\n",
    "        #     print()\n",
    "\n",
    "    if cfg.save_interval != -1 and (\n",
    "        (update_i + 1) % cfg.save_interval == 0 or is_last_update\n",
    "    ):\n",
    "        save_name = osp.join(logger.save_path, f\"ckpt.{update_i}.pth\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"policy\": policy.state_dict(),\n",
    "                \"updater\": updater.state_dict(),\n",
    "                \"update_i\": update_i,\n",
    "            },\n",
    "            save_name,\n",
    "        )\n",
    "        print(f\"Saved to {save_name}\")\n",
    "        eval_info[\"last_ckpt\"] = save_name\n",
    "\n",
    "logger.close()\n",
    "print(eval_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(save_name)\n",
    "updater.load_state_dict(ckpt[\"updater\"], should_load_opt=cfg.resume_training)\n",
    "policy.load_state_dict(ckpt[\"policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.env.env_settings.params.config[\"render_mode\"] = \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2718539/2116432397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"next_observation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2718539/1711999369.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SnakeBcIRL/snake_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Fill the screen with white background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;31m# Draw the snake on the screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "envs = vectorized_env([SnakeEnv(cfg.env.env_settings.params.config) for _ in range(cfg.num_envs)])\n",
    "\n",
    "while True:\n",
    "\n",
    "    obs = envs.reset()\n",
    "    td = TensorDict({\"observation\": obs}, batch_size=cfg.num_envs)\n",
    "    terminated = False \n",
    "    while not terminated : \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            policy.act(td)\n",
    "        next_obs, reward, done, infos = envs.step(td[\"action\"])\n",
    "        envs.render(mode=\"human\")\n",
    "        td[\"next_observation\"] = next_obs\n",
    "        td[\"reward\"] = reward.reshape(-1, 1)\n",
    "\n",
    "        td[\"done\"] = done\n",
    "\n",
    "        td[\"observation\"] = next_obs\n",
    "        terminated = done[0]\n",
    "        # Log to CLI/wandb.\n",
    "        logger.collect_env_step_info(infos)\n",
    "        pygame.time.wait(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcirl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
