{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "from typing import Dict,List\n",
    "\n",
    "import gym.spaces as spaces\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from hydra.utils import instantiate as hydra_instantiate\n",
    "from omegaconf import DictConfig\n",
    "from rl_utils.envs import create_vectorized_envs\n",
    "from rl_utils.logging import Logger\n",
    "from tensordict.tensordict import TensorDict\n",
    "from torchrl.envs.utils import step_mdp\n",
    "from typing import Tuple\n",
    "from imitation_learning.common.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_env import SnakeEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv({})\n",
    "env.render_mode = \"rgb_array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "cfg = yaml.load(open(\"bc-irl-snake.yaml\", 'r'), Loader=yaml.SafeLoader)\n",
    "cfg = DictConfig(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the seed for numpy, python random, and pytorch.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "class vectorized_env():\n",
    "    def __init__(self, envs : List[Env]):\n",
    "        self.envs = envs\n",
    "        self.num_envs = len(self.envs)\n",
    "        self.observation_space = self.envs[0].observation_space\n",
    "        self.action_space = self.envs[0].action_space\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        return torch.tensor([env.reset()[0].tolist() for env in self.envs],dtype=torch.float32)\n",
    "    \n",
    "    def step(self, action) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[Dict]]:\n",
    "        steps = [env.step(action[i]) for i,env in enumerate(self.envs)]\n",
    "        return_value = (torch.tensor([step[0].tolist() for step in steps],dtype=torch.float32),\n",
    "                torch.tensor([step[1] for step in steps],dtype=torch.float32),\n",
    "                torch.tensor([step[2] for step in steps],dtype=torch.bool),\n",
    "                [step[3] for step in steps])\n",
    "        return return_value\n",
    "    \n",
    "    def render(self):\n",
    "        if(self.envs[0].render_mode == \"rgb_array\"):\n",
    "            return [env.render() for env in self.envs]\n",
    "        else:\n",
    "            self.envs[0].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps 300\n",
      "Assigning full prefix 531-3-mgJmuF\n",
      "50000 300 16\n",
      "observations\n",
      "\t torch.Size([160, 452])\n",
      "actions\n",
      "\t torch.Size([160, 4])\n",
      "terminals\n",
      "\t torch.Size([160])\n",
      "next_observations\n",
      "\t torch.Size([160, 452])\n",
      "rewards\n",
      "\t torch.Size([160])\n",
      "infos\n",
      "\t 160\n"
     ]
    }
   ],
   "source": [
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "\n",
    "# Setup the environments\n",
    "set_env_settings = {\n",
    "    k: hydra_instantiate(v) if isinstance(v, DictConfig) else v\n",
    "    for k, v in cfg.env.env_settings.items()\n",
    "}\n",
    "envs = vectorized_env([SnakeEnv({}) for _ in range(cfg.num_envs)])\n",
    "\n",
    "steps_per_update = cfg.num_steps * cfg.num_envs\n",
    "print(\"num_steps\" , cfg.num_steps)\n",
    "num_updates = int(cfg.num_env_steps) // steps_per_update\n",
    "\n",
    "# Set dynamic variables in the config.\n",
    "cfg.obs_shape = envs.observation_space.shape\n",
    "cfg.action_dim = envs.action_space.shape[0]\n",
    "cfg.action_is_discrete = isinstance(cfg.action_dim, spaces.Discrete)\n",
    "cfg.total_num_updates = num_updates\n",
    "\n",
    "logger: Logger = hydra_instantiate(cfg.logger, full_cfg=cfg)\n",
    "policy = hydra_instantiate(cfg.policy)\n",
    "policy = policy.to(device)\n",
    "updater = hydra_instantiate(cfg.policy_updater, policy=policy, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_update = 0\n",
    "if cfg.load_checkpoint is not None:\n",
    "    # Load a checkpoint for the policy/reward. Also potentially resume\n",
    "    # training.\n",
    "    ckpt = torch.load(cfg.load_checkpoint)\n",
    "    updater.load_state_dict(ckpt[\"updater\"], should_load_opt=cfg.resume_training)\n",
    "    if cfg.load_policy:\n",
    "        policy.load_state_dict(ckpt[\"policy\"])\n",
    "    if cfg.resume_training:\n",
    "        start_update = ckpt[\"update_i\"] + 1\n",
    "\n",
    "eval_info = {\"run_name\": logger.run_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "\n",
      "Updates 0, Steps 4800, FPS 374\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -0.05259985327720642\n",
      "    - value_loss: 0.32322481870651243\n",
      "    - action_loss: 0.01661017390433699\n",
      "    - dist_entropy: 5.675754547119141\n",
      "    - irl_loss: 0.3023630678653717\n",
      "\n",
      "Updates 10, Steps 52800, FPS 400\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -2.052572417259216\n",
      "    - value_loss: 0.380956369638443\n",
      "    - action_loss: -0.015241982648149133\n",
      "    - dist_entropy: 5.675553798675537\n",
      "    - irl_loss: 0.2985751897096634\n",
      "\n",
      "Updates 20, Steps 100800, FPS 397\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.182389664649963\n",
      "    - value_loss: 0.35270738899707793\n",
      "    - action_loss: -0.01462983526289463\n",
      "    - dist_entropy: 5.676753997802734\n",
      "    - irl_loss: 0.29719325602054597\n",
      "\n",
      "Updates 30, Steps 148800, FPS 390\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.5279982805252077\n",
      "    - value_loss: 0.11229614168405533\n",
      "    - action_loss: 0.023295605974271893\n",
      "    - dist_entropy: 5.676753997802734\n",
      "    - irl_loss: 0.29484460353851316\n",
      "\n",
      "Updates 40, Steps 196800, FPS 397\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.419733095169067\n",
      "    - value_loss: 0.03931361511349678\n",
      "    - action_loss: -0.0037839610828086733\n",
      "    - dist_entropy: 5.676553726196289\n",
      "    - irl_loss: 0.2936595916748047\n",
      "\n",
      "Updates 50, Steps 244800, FPS 401\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.9027096033096313\n",
      "    - value_loss: 0.03762451186776161\n",
      "    - action_loss: 0.020473546534776687\n",
      "    - dist_entropy: 5.675952911376953\n",
      "    - irl_loss: 0.29251586496829984\n",
      "\n",
      "Updates 60, Steps 292800, FPS 400\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.9697312116622925\n",
      "    - value_loss: 0.00522724948823452\n",
      "    - action_loss: 0.002624287432990968\n",
      "    - dist_entropy: 5.677154064178467\n",
      "    - irl_loss: 0.2917017936706543\n",
      "\n",
      "Updates 70, Steps 340800, FPS 388\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.193122148513794\n",
      "    - value_loss: 0.11571474298834801\n",
      "    - action_loss: 0.010979062202386558\n",
      "    - dist_entropy: 5.677154064178467\n",
      "    - irl_loss: 0.29152480959892274\n",
      "\n",
      "Updates 80, Steps 388800, FPS 399\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -2.2266571521759033\n",
      "    - value_loss: 0.22963834553956985\n",
      "    - action_loss: -0.007132512237876654\n",
      "    - dist_entropy: 5.676952838897705\n",
      "    - irl_loss: 0.29034370481967925\n",
      "\n",
      "Updates 90, Steps 436800, FPS 401\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -2.4914950847625734\n",
      "    - value_loss: 0.0847832553088665\n",
      "    - action_loss: -0.011044764891266823\n",
      "    - dist_entropy: 5.675754547119141\n",
      "    - irl_loss: 0.2916594982147217\n",
      "\n",
      "Updates 100, Steps 484800, FPS 393\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.3172478675842285\n",
      "    - value_loss: 0.09187615662813187\n",
      "    - action_loss: -0.010993998451158404\n",
      "    - dist_entropy: 5.676753997802734\n",
      "    - irl_loss: 0.2934366434812546\n",
      "\n",
      "Updates 110, Steps 532800, FPS 399\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.2603561878204346\n",
      "    - value_loss: 0.06358062252402305\n",
      "    - action_loss: -0.0357222689606715\n",
      "    - dist_entropy: 5.675754547119141\n",
      "    - irl_loss: 0.29608647227287294\n",
      "\n",
      "Updates 120, Steps 580800, FPS 398\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -3.7684473991394043\n",
      "    - value_loss: 0.1030049353837967\n",
      "    - action_loss: -0.00895201526582241\n",
      "    - dist_entropy: 5.6759538650512695\n",
      "    - irl_loss: 0.2979579627513885\n",
      "\n",
      "Updates 130, Steps 628800, FPS 398\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -4.7165937423706055\n",
      "    - value_loss: 0.30659896433353423\n",
      "    - action_loss: -0.023373654030729087\n",
      "    - dist_entropy: 5.676553249359131\n",
      "    - irl_loss: 0.2968735367059708\n",
      "\n",
      "Updates 140, Steps 676800, FPS 394\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -4.958254337310791\n",
      "    - value_loss: 0.11138521432876587\n",
      "    - action_loss: -0.0489405850879848\n",
      "    - dist_entropy: 5.676952838897705\n",
      "    - irl_loss: 0.29844223856925967\n",
      "\n",
      "Updates 150, Steps 724800, FPS 392\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -5.10774040222168\n",
      "    - value_loss: 0.05506519936025143\n",
      "    - action_loss: 0.006053727841936052\n",
      "    - dist_entropy: 5.677753925323486\n",
      "    - irl_loss: 0.29802493155002596\n",
      "\n",
      "Updates 160, Steps 772800, FPS 387\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -5.2825908184051515\n",
      "    - value_loss: 0.10982287228107453\n",
      "    - action_loss: 0.008514706254936754\n",
      "    - dist_entropy: 5.678354263305664\n",
      "    - irl_loss: 0.2998466283082962\n",
      "\n",
      "Updates 170, Steps 820800, FPS 395\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -4.9488475799560545\n",
      "    - value_loss: 0.3623093694448471\n",
      "    - action_loss: -0.03391634964791592\n",
      "    - dist_entropy: 5.677753925323486\n",
      "    - irl_loss: 0.29985500276088717\n",
      "\n",
      "Updates 180, Steps 868800, FPS 397\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -4.978062295913697\n",
      "    - value_loss: 0.2611739918589592\n",
      "    - action_loss: 0.014524012757465243\n",
      "    - dist_entropy: 5.6787543296813965\n",
      "    - irl_loss: 0.29983683228492736\n",
      "\n",
      "Updates 190, Steps 916800, FPS 396\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -6.341537952423096\n",
      "    - value_loss: 1.1286212861537934\n",
      "    - action_loss: -0.010763838328421116\n",
      "    - dist_entropy: 5.678753852844238\n",
      "    - irl_loss: 0.2954308956861496\n",
      "\n",
      "Updates 200, Steps 964800, FPS 392\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -6.173650741577148\n",
      "    - value_loss: 1.0991097688674927\n",
      "    - action_loss: 0.002466834057122469\n",
      "    - dist_entropy: 5.678354263305664\n",
      "    - irl_loss: 0.29445287585258484\n",
      "\n",
      "Updates 207, Steps 998400, FPS 392\n",
      "Over the last 0 episodes:\n",
      "    - inferred_episode_reward: -6.063787460327148\n",
      "    - value_loss: 0.8720228731632232\n",
      "    - action_loss: -0.002504969423171133\n",
      "    - dist_entropy: 5.678153991699219\n",
      "    - irl_loss: 0.2949557214975357\n",
      "Saved to ./data/checkpoints/531-3-xuHQRv/ckpt.207.pth\n",
      "{'run_name': '531-3-xuHQRv', 'last_ckpt': './data/checkpoints/531-3-xuHQRv/ckpt.207.pth'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "obs = envs.reset()\n",
    "td = TensorDict({\"observation\": obs}, batch_size=[cfg.num_envs])\n",
    "# Storage for the rollouts\n",
    "storage_td = TensorDict({}, batch_size=[cfg.num_envs, cfg.num_steps], device=device)\n",
    "print(num_updates)\n",
    "for update_i in range(start_update, num_updates):\n",
    "    is_last_update = update_i == num_updates - 1\n",
    "    for step_idx in range(cfg.num_steps):\n",
    "        # Collect experience.\n",
    "        with torch.no_grad():\n",
    "            policy.act(td)\n",
    "        next_obs, reward, done, infos = envs.step(td[\"action\"])\n",
    "\n",
    "        td[\"next_observation\"] = next_obs\n",
    "        for env_i, info in enumerate(infos):\n",
    "            if \"final_obs\" in info:\n",
    "                td[\"next_observation\"][env_i] = info[\"final_obs\"]\n",
    "        td[\"reward\"] = reward.reshape(-1, 1)\n",
    "\n",
    "        td[\"done\"] = done\n",
    "        storage_td[:, step_idx] = td\n",
    "        td[\"observation\"] = next_obs\n",
    "        # Log to CLI/wandb.\n",
    "        logger.collect_env_step_info(infos)\n",
    "\n",
    "    # Call method specific update function\n",
    "    updater.update(policy, storage_td, logger, envs=envs)\n",
    "\n",
    "\n",
    "    if cfg.log_interval != -1 and (\n",
    "        update_i % cfg.log_interval == 0 or is_last_update\n",
    "    ):\n",
    "        logger.interval_log(update_i, steps_per_update * (update_i + 1))\n",
    "\n",
    "    if cfg.save_interval != -1 and (\n",
    "        (update_i + 1) % cfg.save_interval == 0 or is_last_update\n",
    "    ):\n",
    "        save_name = osp.join(logger.save_path, f\"ckpt.{update_i}.pth\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"policy\": policy.state_dict(),\n",
    "                \"updater\": updater.state_dict(),\n",
    "                \"update_i\": update_i,\n",
    "            },\n",
    "            save_name,\n",
    "        )\n",
    "        print(f\"Saved to {save_name}\")\n",
    "        eval_info[\"last_ckpt\"] = save_name\n",
    "\n",
    "logger.close()\n",
    "print(eval_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./data/checkpoints/531-3-xuHQRv/ckpt.207.pth\")\n",
    "updater.load_state_dict(ckpt[\"updater\"], should_load_opt=cfg.resume_training)\n",
    "policy.load_state_dict(ckpt[\"policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = vectorized_env([SnakeEnv({\"render_mode\":\"human\"}) for _ in range(1)])\n",
    "obs = envs.reset()\n",
    "td = TensorDict({\"observation\": obs}, batch_size=[1])\n",
    "# Storage for the rollouts\n",
    "# storage_td = TensorDict({}, batch_size=[cfg.num_envs, cfg.num_steps], device=device)\n",
    "# print(num_updates)\n",
    "# # for update_i in range(start_update, num_updates):\n",
    "# #     is_last_update = update_i == num_updates - 1\n",
    "# for step_idx in range(cfg.num_steps):\n",
    "#     # Collect experience.\n",
    "#     with torch.no_grad():\n",
    "#         policy.act(td)\n",
    "#     next_obs, reward, done, infos = envs.step(td[\"action\"])\n",
    "    \n",
    "#     td[\"next_observation\"] = next_obs\n",
    "#     for env_i, info in enumerate(infos):\n",
    "#         if \"final_obs\" in info:\n",
    "#             td[\"next_observation\"][env_i] = info[\"final_obs\"]\n",
    "#     td[\"reward\"] = reward.reshape(-1, 1)\n",
    "\n",
    "#     td[\"done\"] = done\n",
    "#     storage_td[:, step_idx] = td\n",
    "#     td[\"observation\"] = next_obs\n",
    "#     # Log to CLI/wandb.\n",
    "#     logger.collect_env_step_info(infos)\n",
    "\n",
    "\n",
    "done = False \n",
    "while not done : \n",
    "    with torch.no_grad():\n",
    "        policy.act(td)\n",
    "    next_obs, reward, done, infos = envs.step(td[\"action\"])\n",
    "    envs.render(mode=\"human\")\n",
    "    td[\"next_observation\"] = next_obs\n",
    "    td[\"reward\"] = reward.reshape(-1, 1)\n",
    "\n",
    "    td[\"done\"] = done\n",
    "\n",
    "    td[\"observation\"] = next_obs\n",
    "    # Log to CLI/wandb.\n",
    "    logger.collect_env_step_info(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcirl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
